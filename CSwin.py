import torch
import torch.nn as nn

import numpy as np

from timm.models.layers import trunc_normal_
from einops.layers.torch import Rearrange
from CustomCSWinBlock import CustomCSWinBlock
import torch.utils.checkpoint as checkpoint
from MergeBlock import MergeBlock


class CSwin(nn.Module):
    """
    Vision Transformer with support for patch or hybrid CNN input stage
    """

    def __init__(
        self,
        mode:str,
        img_size=224,
        patch_size=16,
        in_chans=3,
        num_classes=100,
        embed_dim=96,
        depth=[2, 2, 6, 2],
        split_size=[3, 5, 7],
        num_heads=12,
        mlp_ratio=4.0,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        hybrid_backbone=None,
        norm_layer=nn.LayerNorm,
        use_chk=False,
    ):
        super().__init__()
        self.use_chk = use_chk
        self.num_classes = num_classes
        self.num_features = (
            self.embed_dim
        ) = embed_dim  # num_features for consistency with other models
        heads = num_heads

        self.stage1_conv_embed = nn.Sequential(
            nn.Conv2d(in_chans, embed_dim, 7, 4, 2),
        )
        self.stage1_reshape = nn.Sequential(
            Rearrange("b c h w -> b (h w) c", h=img_size // 4, w=img_size // 4),
            nn.LayerNorm(embed_dim),
        )

        curr_dim = embed_dim
        dpr = [
            x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))
        ]  # stochastic depth decay rule
        self.stage1 = nn.ModuleList(
            [
                CustomCSWinBlock(
                    mode=mode,
                    input_dim=curr_dim,
                    num_heads=heads[0],
                    patch_size=img_size // 4,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    split_size=split_size[0],
                    dropout=drop_rate,
                    attention_dropout=attn_drop_rate,
                    drop_path=dpr[i],
                    normalization_layer=norm_layer,
                )
                for i in range(depth[0])
            ]
        )

        self.merge1 = MergeBlock(curr_dim, curr_dim * 2)
        curr_dim = curr_dim * 2
        self.stage2 = nn.ModuleList(
            [
                CustomCSWinBlock(
                    mode=mode,
                    input_dim=curr_dim,
                    num_heads=heads[1],
                    patch_size=img_size // 8,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    split_size=split_size[1],
                    dropout=drop_rate,
                    attention_dropout=attn_drop_rate,
                    drop_path=dpr[np.sum(depth[:1]) + i],
                    normalization_layer=norm_layer,
                )
                for i in range(depth[1])
            ]
        )

        self.merge2 = MergeBlock(curr_dim, curr_dim * 2)
        curr_dim = curr_dim * 2
        temp_stage3 = []
        temp_stage3.extend(
            [
                CustomCSWinBlock(
                    mode=mode,
                    input_dim=curr_dim,
                    num_heads=heads[2],
                    patch_size=img_size // 16,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    split_size=split_size[2],
                    dropout=drop_rate,
                    attention_dropout=attn_drop_rate,
                    drop_path=dpr[np.sum(depth[:2]) + i],
                    normalization_layer=norm_layer,
                )
                for i in range(depth[2])
            ]
        )

        self.stage3 = nn.ModuleList(temp_stage3)

        self.merge3 = MergeBlock(curr_dim, curr_dim * 2)
        curr_dim = curr_dim * 2
        self.stage4 = nn.ModuleList(
            [
                CustomCSWinBlock(
                    mode=mode,
                    input_dim=curr_dim,
                    num_heads=heads[3],
                    patch_size=img_size // 32,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    split_size=split_size[-1],
                    dropout=drop_rate,
                    attention_dropout=attn_drop_rate,
                    drop_path=dpr[np.sum(depth[:-1]) + i],
                    normalization_layer=norm_layer,
                    last_stage=True,
                )
                for i in range(depth[-1])
            ]
        )

        self.norm = norm_layer(curr_dim)
        # Classifier head
        self.head = (
            nn.Linear(curr_dim, num_classes) if num_classes > 0 else nn.Identity()
        )

        trunc_normal_(self.head.weight, std=0.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {"pos_embed", "cls_token"}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=""):
        if self.num_classes != num_classes:
            print("reset head to", num_classes)
            self.num_classes = num_classes
            self.head = (
                nn.Linear(self.out_dim, num_classes)
                if num_classes > 0
                else nn.Identity()
            )
            self.head = self.head.cuda()
            trunc_normal_(self.head.weight, std=0.02)
            if self.head.bias is not None:
                nn.init.constant_(self.head.bias, 0)

    def forward_features(self, x):
        B = x.shape[0]
        x = self.stage1_conv_embed(x)
        # print(x.shape)
        x = self.stage1_reshape(x)
        for blk in self.stage1:
            if self.use_chk:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        for pre, blocks in zip(
            [self.merge1, self.merge2, self.merge3],
            [self.stage2, self.stage3, self.stage4],
        ):
            x = pre(x)
            for blk in blocks:
                if self.use_chk:
                    x = checkpoint.checkpoint(blk, x)
                else:
                    x = blk(x)
        x = self.norm(x)
        return torch.mean(x, dim=1)

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
